{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"14pDaJGxUWXJCnMdPW8ycj6bvy85UeO0p","authorship_tag":"ABX9TyMtndLSkeH/6WpYgrVreBd3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Let's build a transformer based, character level language model."],"metadata":{"id":"O4XfPmk6i_dY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"AGhmuxO7zVi0","executionInfo":{"status":"ok","timestamp":1706211512945,"user_tz":300,"elapsed":4752,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"k4IBVKv_ip6E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706211513442,"user_tz":300,"elapsed":499,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"2af366e0-a776-495f-c899-b861b761b2bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"]}],"source":["with open(r'/content/drive/MyDrive/makemore/GPTFromScratch/input.txt','r',encoding='utf-8') as f:\n","  text = f.read()\n","print(text[:100])"]},{"cell_type":"code","source":["chars = sorted(list(set(''.join(text))))\n","vocab_size = len(chars)\n","vocab_size,''.join(chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"118m5oYr0wTg","executionInfo":{"status":"ok","timestamp":1706211513606,"user_tz":300,"elapsed":166,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"dd3a29af-0c57-43b7-b291-aee1f8b566a7"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# tokenize: convert the raw text as string to some sequence of integers\n","stoi = {s:i for i,s in enumerate(chars)}\n","itos = {i:s for i,s in enumerate(chars)}\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","print(encode('hi there'))\n","print(decode(encode('hi there')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48AXVDfV1hip","executionInfo":{"status":"ok","timestamp":1706211513606,"user_tz":300,"elapsed":3,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"4b095a24-2df4-4bf3-9d96-62592236af1f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[46, 47, 1, 58, 46, 43, 56, 43]\n","hi there\n"]}]},{"cell_type":"code","source":["data = torch.tensor(encode(text),dtype=torch.long)\n","print(data.shape,data.dtype)\n","print(data[:100])\n","# now the data is a single, long sequence of integers\n","# they are the encodings of the raw text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1qlPQaH3lkM","executionInfo":{"status":"ok","timestamp":1706211513894,"user_tz":300,"elapsed":290,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"10c3066f-8d01-49c6-92b9-f4ed19e61cb2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"]}]},{"cell_type":"code","source":["# train, validation split\n","n = int(len(data)*0.9)\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"eQOU9FqV388M","executionInfo":{"status":"ok","timestamp":1706211513894,"user_tz":300,"elapsed":5,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f'when input is {context}, the target: {target}')\n","# so a single sample contains several (block_size) examples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPMfnzdh4l-8","executionInfo":{"status":"ok","timestamp":1706211513894,"user_tz":300,"elapsed":4,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"e9b8f19a-344d-4c02-d91b-224e28e42f6f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([18]), the target: 47\n","when input is tensor([18, 47]), the target: 56\n","when input is tensor([18, 47, 56]), the target: 57\n","when input is tensor([18, 47, 56, 57]), the target: 58\n","when input is tensor([18, 47, 56, 57, 58]), the target: 1\n","when input is tensor([18, 47, 56, 57, 58,  1]), the target: 15\n","when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target: 47\n","when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target: 58\n"]}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","batch_size = 4\n","\n","def get_batch(split):\n","  # generate a small batch of data of inputs x and targets y\n","  data = train_data if split=='train' else val_data\n","  ix = torch.randint(0,len(data)-block_size,(batch_size,))\n","  x = torch.stack([data[i:i+block_size] for i in ix])\n","  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n","  return x,y\n","\n","xb,yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('------')\n","for b in range(batch_size):\n","  for t in range(block_size):\n","    context = xb[b,:t+1]\n","    target = yb[b,t]\n","    print(f'when input is {context.tolist()} the target: {target}')\n","# the input and the output are both sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjDyNHTd4l8d","executionInfo":{"status":"ok","timestamp":1706211514022,"user_tz":300,"elapsed":131,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"eb0e1c2a-2aaf-4450-c4aa-a9b2469b58bc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n","        [ 1, 58, 46, 43, 56, 43,  1, 41],\n","        [17, 26, 15, 17, 10,  0, 32, 53],\n","        [57, 58,  6,  1, 61, 47, 58, 46]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n","        [58, 46, 43, 56, 43,  1, 41, 39],\n","        [26, 15, 17, 10,  0, 32, 53,  1],\n","        [58,  6,  1, 61, 47, 58, 46,  0]])\n","------\n","when input is [57] the target: 1\n","when input is [57, 1] the target: 46\n","when input is [57, 1, 46] the target: 47\n","when input is [57, 1, 46, 47] the target: 57\n","when input is [57, 1, 46, 47, 57] the target: 1\n","when input is [57, 1, 46, 47, 57, 1] the target: 50\n","when input is [57, 1, 46, 47, 57, 1, 50] the target: 53\n","when input is [57, 1, 46, 47, 57, 1, 50, 53] the target: 60\n","when input is [1] the target: 58\n","when input is [1, 58] the target: 46\n","when input is [1, 58, 46] the target: 43\n","when input is [1, 58, 46, 43] the target: 56\n","when input is [1, 58, 46, 43, 56] the target: 43\n","when input is [1, 58, 46, 43, 56, 43] the target: 1\n","when input is [1, 58, 46, 43, 56, 43, 1] the target: 41\n","when input is [1, 58, 46, 43, 56, 43, 1, 41] the target: 39\n","when input is [17] the target: 26\n","when input is [17, 26] the target: 15\n","when input is [17, 26, 15] the target: 17\n","when input is [17, 26, 15, 17] the target: 10\n","when input is [17, 26, 15, 17, 10] the target: 0\n","when input is [17, 26, 15, 17, 10, 0] the target: 32\n","when input is [17, 26, 15, 17, 10, 0, 32] the target: 53\n","when input is [17, 26, 15, 17, 10, 0, 32, 53] the target: 1\n","when input is [57] the target: 58\n","when input is [57, 58] the target: 6\n","when input is [57, 58, 6] the target: 1\n","when input is [57, 58, 6, 1] the target: 61\n","when input is [57, 58, 6, 1, 61] the target: 47\n","when input is [57, 58, 6, 1, 61, 47] the target: 58\n","when input is [57, 58, 6, 1, 61, 47, 58] the target: 46\n","when input is [57, 58, 6, 1, 61, 47, 58, 46] the target: 0\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","  def __init__(self,vocab_size):\n","    super().__init__()\n","    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n","    # link of nn.embedding: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","    # note: this lookup table is trainable, and should be trained\n","\n","  def forward(self,idx,targets=None):\n","    # idx and targets are both (B,T) tensor of integers\n","    # NOTE: both B and T here is not fixed, even though during training phase we use T=block_size, during generating phase T can be any integer number\n","    logits = self.token_embedding_table(idx) # (B,T,C)\n","    # loss = F.cross_entropy(logits,targets) # (B,T)\n","    # note: the shape of logits and the shape of target don't match, so this won't work\n","    # cross_entropy function asks the Channel dim to be the second dim\n","    # link of F.cross_entropy: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n","    if targets == None: # this is for the generating phase, where we don't provide the targets\n","      loss = None\n","    else:\n","      # B, T, C = logits.shape\n","      # l = logits.view(B*T,C)\n","      # targets = targets.view(B*T)\n","      # Honestly, what Andrej did here is hard for me to understand, because I can't understand why we can just multiply the batch dim and the time dim\n","      # So I transpose the dim to make it more acceptable to me, and of course this will get the same answer\n","      l = torch.transpose(logits,1,2)\n","      loss = F.cross_entropy(l,targets)\n","    return logits,loss\n","\n","  def generate(self,idx,max_new_tokens):\n","    # idx is (B,T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","      # get the predictions\n","      logits, loss = self(idx) # NOTE: this means that during this for loop, the model sees all the previous input to make the next prediction\n","      # focus only on the last time step\n","      logits = logits[:,-1,:] # (B,C)\n","      # apply softmax to get probabilities\n","      probs = F.softmax(logits,dim=-1) # (B,C)\n","      # sample from the distribution\n","      idx_next = torch.multinomial(probs,num_samples=1,replacement=True)\n","      # append sampled index to the running sequence\n","      idx = torch.cat((idx,idx_next),dim=1) # (B,T+1)\n","    return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits,loss = m(xb,yb)\n","print(logits.shape) # [batch_size, block_size, vocab_size]\n","print(yb.shape) # [batch_size, block_size]\n","print(loss)\n","\n","idx = torch.zeros((1,1),dtype=torch.long) # (1,1) means there is a single batch, and the length of the input sequence is 0\n","print(decode(m.generate(idx,max_new_tokens=100)[0].tolist()))"],"metadata":{"id":"fhXj6KCC4l5u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706211514140,"user_tz":300,"elapsed":119,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"3b0baff9-3028-4f60-b942-c8679e6a213a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 8, 65])\n","torch.Size([4, 8])\n","tensor(4.7013, grad_fn=<NllLoss2DBackward0>)\n","\n","Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"]}]},{"cell_type":"code","source":["# now let's train the model\n","# create a pytorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)\n","batch_size = 32\n","for steps in range(10000):\n","  # sample a batch of data\n","  xb,yb = get_batch('train')\n","\n","  # evaluate the loss\n","  logits,loss = m(xb,yb)\n","  optimizer.zero_grad(set_to_none=True) # link: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n","  loss.backward()\n","  optimizer.step()\n","\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jjxPxHQVcX6","executionInfo":{"status":"ok","timestamp":1706211537359,"user_tz":300,"elapsed":23221,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"a8ba4f0e-951f-4b33-a19d-0be5114867b1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2.572751045227051\n"]}]},{"cell_type":"code","source":["idx = torch.zeros((1,1),dtype=torch.long)\n","print(decode(m.generate(idx,max_new_tokens=100)[0].tolist()))\n","# well, it's still pretty bad, but makes more sense than the untrained model\n","# after all we cannot expect much from this simple bigram model :("],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdbqYCg3VcVi","executionInfo":{"status":"ok","timestamp":1706211537359,"user_tz":300,"elapsed":5,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"f61c0f57-2ab3-4465-bfdb-842ae0be664b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Iyoteng h hasbe pave pirance\n","Rie hicomyonthar's\n","Plinseard ith henoure wounonthioneir thondy, y helti\n"]}]},{"cell_type":"markdown","source":["# The mathematical trick in self-attention"],"metadata":{"id":"o82RK0vnIWcQ"}},{"cell_type":"code","source":["# consider the following toy example\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"id":"CbKQCZ0PVcTU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706216961742,"user_tz":300,"elapsed":105,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"ec2f882c-19e4-4abb-c4b5-de72fdee8f5f"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# we want tokens in the sequence to communicate with each other\n","# in this case, we want the token to communicate with all the previous tokens\n","# let's use this simple example where the current token is the mean of all the previous tokens and itself\n","# i.e. we want x[b,t] = mean_{i<=t} x[b,i]\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","  for t in range(T):\n","    xprev = x[b,:t+1] # (t,C)\n","    xbow[b,t] = torch.mean(xprev,0)"],"metadata":{"id":"VHs-uTOiVcRD","executionInfo":{"status":"ok","timestamp":1706216962950,"user_tz":300,"elapsed":97,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["x[0] # the first batch"],"metadata":{"id":"rGkPZKm8VcO0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706211949816,"user_tz":300,"elapsed":91,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"de9102c5-88c3-4ec5-efa9-157cc7e5d8dc"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1808, -0.0700],\n","        [-0.3596, -0.9152],\n","        [ 0.6258,  0.0255],\n","        [ 0.9545,  0.0643],\n","        [ 0.3612,  1.1679],\n","        [-1.3499, -0.5102],\n","        [ 0.2360, -0.2398],\n","        [-0.9211,  1.5433]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["xbow[0]"],"metadata":{"id":"yJiIdEMFVcMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706211965177,"user_tz":300,"elapsed":119,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"940e1a37-5b28-4df0-805e-d8b2189d76f0"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1808, -0.0700],\n","        [-0.0894, -0.4926],\n","        [ 0.1490, -0.3199],\n","        [ 0.3504, -0.2238],\n","        [ 0.3525,  0.0545],\n","        [ 0.0688, -0.0396],\n","        [ 0.0927, -0.0682],\n","        [-0.0341,  0.1332]])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# this code does what we want, but it's very inefficient with two for loops\n","# let's see how we can do it using matrix multiplication\n","torch.manual_seed(42)\n","a = torch.ones(3,3)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a@b\n","print('a=')\n","print(a)\n","print('b=')\n","print(b)\n","print('c=')\n","print(c)\n","# now, c is the 'time-wise' sum of b"],"metadata":{"id":"xMbb5Fj1VcKR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706212148923,"user_tz":300,"elapsed":103,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"fa644b5b-9936-4919-c35e-ce819e7dc108"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","c=\n","tensor([[14., 16.],\n","        [14., 16.],\n","        [14., 16.]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","a = torch.tril(torch.ones(3,3)) # link: https://pytorch.org/docs/stable/generated/torch.tril.html\n","b = torch.randint(0,10,(3,2)).float()\n","c = a@b\n","print('a=')\n","print(a)\n","print('b=')\n","print(b)\n","print('c=')\n","print(c)\n","# with this low triagnle matrix a, c is the 'time-wise' sum of previous elements of b"],"metadata":{"id":"LaDTkbNPVcII","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706212256083,"user_tz":300,"elapsed":115,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"94235586-4f2a-468a-c255-777f801eb3a7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1., 0., 0.],\n","        [1., 1., 0.],\n","        [1., 1., 1.]])\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","c=\n","tensor([[ 2.,  7.],\n","        [ 8., 11.],\n","        [14., 16.]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","a = torch.tril(torch.ones(3,3))\n","a = a / a.sum(dim=1,keepdim=True)   #\n","b = torch.randint(0,10,(3,2)).float()\n","c = a@b\n","print('a=')\n","print(a)\n","print('b=')\n","print(b)\n","print('c=')\n","print(c)"],"metadata":{"id":"uy1VU8WXVcFr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706212410927,"user_tz":300,"elapsed":147,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"dc959e43-7bb2-4461-fd63-7349b3f070db"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"code","source":["# we can generalize this, add the batch size\n","wei = torch.tril(torch.ones(T,T))\n","wei = wei / wei.sum(1,keepdim=True)\n","xbow2 = wei @ x # wei now is broadcasting as (B,T,T), and x is (B,T,C), this multiplication will give us (B,T,C)\n","xbow[0],xbow2[0]\n","# note: here we assume that the communication among time steps is taking the average, however in transformer we won't use such a simple 'communication'\n","# therefore, wei won't be torch.tril(torch.ones(T,T))"],"metadata":{"id":"KRykweWmVcDT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706217123944,"user_tz":300,"elapsed":91,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"0fb5eef4-d179-4c61-ecf5-1277b0507a16"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.1808, -0.0700],\n","         [-0.0894, -0.4926],\n","         [ 0.1490, -0.3199],\n","         [ 0.3504, -0.2238],\n","         [ 0.3525,  0.0545],\n","         [ 0.0688, -0.0396],\n","         [ 0.0927, -0.0682],\n","         [-0.0341,  0.1332]]),\n"," tensor([[ 0.1808, -0.0700],\n","         [-0.0894, -0.4926],\n","         [ 0.1490, -0.3199],\n","         [ 0.3504, -0.2238],\n","         [ 0.3525,  0.0545],\n","         [ 0.0688, -0.0396],\n","         [ 0.0927, -0.0682],\n","         [-0.0341,  0.1332]]))"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# another version, which is what we will use\n","tril = torch.tril(torch.ones(T,T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril==0,float('-inf')) # link: https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_\n","print(wei)\n","wei = F.softmax(wei,dim=-1)\n","print(wei)\n","xbow3 = wei @ x\n","xbow3[0]\n","# we can interprete wei as: how much of every time steps we want to integrate into our current step\n","# and the -inf means that the current time step cannot communicate to the future"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCsIrP7gdXvq","executionInfo":{"status":"ok","timestamp":1706217452637,"user_tz":300,"elapsed":132,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"1c092bab-7dbb-49e3-e4a0-4269eadf1adf"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n","        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n","        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n","        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n","        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n","        [0., 0., 0., 0., 0., 0., -inf, -inf],\n","        [0., 0., 0., 0., 0., 0., 0., -inf],\n","        [0., 0., 0., 0., 0., 0., 0., 0.]])\n","tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n","        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n","        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n","        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1808, -0.0700],\n","        [-0.0894, -0.4926],\n","        [ 0.1490, -0.3199],\n","        [ 0.3504, -0.2238],\n","        [ 0.3525,  0.0545],\n","        [ 0.0688, -0.0396],\n","        [ 0.0927, -0.0682],\n","        [-0.0341,  0.1332]])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# let's modify the model\n","# this model only adds a positional encoding and a linear layer after token embedding\n","n_embd = 32\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","class BigramLanguageModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.token_embedding_table = nn.Embedding(vocab_size,n_embd) # embedding layer\n","    self.position_embedding_table = nn.Embedding(block_size,n_embd) # positional encoding\n","    self.lm_head = nn.Linear(n_embd,vocab_size)         # a linear layer\n","\n","  def forward(self,idx,targets=None):\n","    B,T = idx.shape\n","    tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n","    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) # (T,n_embd)\n","    x = tok_emb + pos_emb # (B,T,n_embd)\n","    logits = self.lm_head(x) # (B,T,vocab_size)\n","    if targets == None:\n","      loss = None\n","    else:\n","      l = torch.transpose(logits,1,2)\n","      loss = F.cross_entropy(l,targets)\n","\n","    return logits,loss\n","  def generate(self,idx,max_new_tokens):\n","    for _ in range(max_new_tokens):\n","      logits,loss = self(idx)\n","      logits = logits[:,-1,:]\n","      probs = F.softmax(logits,-1)\n","      idx_next = torch.multinomial(probs,num_samples=1,replacement=True)\n","      idx = torch.cat([idx,idx_next],dim=1)\n","    return idx\n","\n","m = BigramLanguageModel()\n","context = torch.zeros((1,1),dtype=torch.long)\n","print(decode(m.generate(context,100)[0].tolist()))\n","\n","# this will cause an error\n","# we will fix it later"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"Z5GBP5YKggq-","executionInfo":{"status":"error","timestamp":1706219107029,"user_tz":300,"elapsed":239,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"7a26e726-c886-41b7-f6fc-6499f5490d83"},"execution_count":44,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index out of range in self","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-7f2ace93b347>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-7f2ace93b347>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-7f2ace93b347>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T,n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B,T,n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","source":["# self-attention\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32\n","x = torch.randn(B,T,C)\n","\n","# let's see a single head perform self-attention\n","head_size = 16\n","key = nn.Linear(C,head_size,bias=False)\n","query = nn.Linear(C,head_size,bias=False)\n","value = nn.Linear(C,head_size)\n","k = key(x)  # B,T,head_size\n","q = query(x) # B,T,head_size\n","v = value(x) # B,T,head_size\n","\n","wei = q @ k.transpose(-2,-1) * head_size**-0.5 # (B,T,16) @ (B,16,T) ---> (B,T,T)\n","tril = torch.tril(torch.ones(T,T))\n","wei = wei.masked_fill(tril==0,float('-inf'))\n","wei = F.softmax(wei,-1)\n","out = wei @ v\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5z3bp4kYggoq","executionInfo":{"status":"ok","timestamp":1706219915990,"user_tz":300,"elapsed":101,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"89145c14-39f1-458f-c37f-d38e5014fb39"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["Notes:\n","  * Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","  * There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n","  * Each examples across batch dimensions is of course processed completely independently and never 'talk' to each other.\n","  * In an 'encoder' attention block, just delete the single line that does masking with tril, allowing all tokens to communicate. The block here is called a 'decoder' attention block because it has triangular masking, and is usually used in autoregressive settings.\n","  * 'self-attention' just means that the keys and values are produced from the same sources as queries, In 'cross-attention', the get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","  * 'scaled' attention additional diveds wei by 1/sqrt(head_size). This makes it so when input Q,K are unit, variance, wei will be unit variance too and softmax will stay diffuse and not saturate too much."],"metadata":{"id":"oNRr31FBon_-"}},{"cell_type":"code","source":["# here is the illustration about the 'scaled'\n","k = torch.randn(B,T,head_size)\n","q = torch.randn(B,T,head_size)\n","wei = q @ k.transpose(-1,-2)\n","k.var(),q.var(),wei.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLwriWBUggme","executionInfo":{"status":"ok","timestamp":1706220689885,"user_tz":300,"elapsed":115,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"8a8fc0d8-2c47-47f4-cf5f-07768c86687e"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(1.0580), tensor(1.0693), tensor(17.7734))"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["wei = q @ k.transpose(-1,-2) * head_size**-0.5\n","wei.var()\n","# we can see that the variance is roughly preserved"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDX-CtcDggj5","executionInfo":{"status":"ok","timestamp":1706220726890,"user_tz":300,"elapsed":92,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"7fb1a1e7-9f65-4dc5-fdbe-6abeb640d52c"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.1108)"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["# why we want to preserve the variance\n","torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5]),dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVMPPzDtgghy","executionInfo":{"status":"ok","timestamp":1706220803029,"user_tz":300,"elapsed":103,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"c7415820-b561-4478-f2dc-2f3a36bd7724"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5])*8,dim=-1)\n","# we can see that the output of softmax 'sharpens' to the large number"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_JGnpmdggfk","executionInfo":{"status":"ok","timestamp":1706220866632,"user_tz":300,"elapsed":109,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}},"outputId":"4f980c53-a172-4d56-92a9-f5aa67e5ed1d"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["class Head(nn.Module):\n","    def __init__(self):\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # link: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        q = self.query(x) # B,T,head_size\n","        k = self.key(x)\n","        v = self.value(x)\n","        wei = q @ k.transpose(-1, -2) * head_size ** -0.5 # B,T,T\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n","        wei = F.softmax(wei, dim=-1)\n","        out = wei @ v # B,T,head_size\n","        return out"],"metadata":{"id":"LatXYEyoggdG","executionInfo":{"status":"ok","timestamp":1706223298508,"user_tz":300,"elapsed":134,"user":{"displayName":"Qiao Qin","userId":"13421871949493803534"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["# now let's add a head to the language model\n","class BigramLanguageModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.token_embedding_table = nn.Embedding(vocab_size,n_embd) # embedding layer\n","    self.position_embedding_table = nn.Embedding(block_size,n_embd) # positional encoding\n","    self.sa_head = Head(n_embd)                 # self-attention\n","    self.lm_head = nn.Linear(n_embd,vocab_size)         # a linear layer\n","\n","  def forward(self,idx,targets=None):\n","    B,T = idx.shape\n","    tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n","    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) # (T,n_embd)\n","    x = tok_emb + pos_emb # (B,T,n_embd)\n","    x = self.sa_head(x) # apply one head of self-attention, (B,T,C)\n","    logits = self.lm_head(x) # (B,T,vocab_size)\n","    if targets == None:\n","      loss = None\n","    else:\n","      l = torch.transpose(logits,1,2)\n","      loss = F.cross_entropy(l,targets)\n","\n","    return logits,loss\n","  def generate(self,idx,max_new_tokens):\n","    for _ in range(max_new_tokens):\n","      logits,loss = self(idx[:,-block_size])    # the length of the sequence should not be bigger than block_size\n","      logits = logits[:,-1,:]\n","      probs = F.softmax(logits,-1)\n","      idx_next = torch.multinomial(probs,num_samples=1,replacement=True)\n","      idx = torch.cat([idx,idx_next],dim=1)\n","    return idx"],"metadata":{"id":"rxH6od2jggaN"},"execution_count":null,"outputs":[]}]}